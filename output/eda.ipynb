{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 rouge1    rouge2     rougeL  bert_precision  bert_recall  \\\n",
      "Markov Chain  33.720140  6.190105  14.607146        0.823217     0.827640   \n",
      "Our Approach  34.130364  6.392678  14.571875        0.821687     0.825116   \n",
      "LLM Pure      23.990682  5.447992  11.996699        0.832335     0.829389   \n",
      "\n",
      "               bert_f1  first_order_coherence  second_order_coherence  \n",
      "Markov Chain  0.825415               0.852806                0.850670  \n",
      "Our Approach  0.823393               0.863621                0.862106  \n",
      "LLM Pure      0.830805               0.729531                0.730952  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV files\n",
    "file_paths = {\n",
    "    \"wo_markov_chain\": r\"wo_markov_approach\\summarization_results_test\\wp_markov_chain__approach_summarization_results_test.csv\",\n",
    "    \"our_approach\": r\"our_approach\\summarization_results_test\\our_approach_summarization_results_test.csv\",\n",
    "    \"llm_pure\": r\"llm_approach\\summarization_results_test\\llm_pure_approach_summarization_results_test.csv\"\n",
    "}\n",
    "\n",
    "# Read the CSV files\n",
    "markov_chain_df = pd.read_csv(file_paths[\"wo_markov_chain\"])\n",
    "our_approach_df = pd.read_csv(file_paths[\"our_approach\"])\n",
    "llm_pure_df = pd.read_csv(file_paths[\"llm_pure\"])\n",
    "\n",
    "# Function to extract and calculate average scores from nested dictionaries, handling NaN values\n",
    "def extract_average_scores_handling_nan(df, score_column, keys):\n",
    "    scores = {key: [] for key in keys}\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            score_dict = eval(row[score_column].replace('nan', 'np.nan'))  # Replace 'nan' with np.nan for safe evaluation\n",
    "            for key in keys:\n",
    "                scores[key].append(score_dict.get(key, np.nan))\n",
    "        except:\n",
    "            for key in keys:\n",
    "                scores[key].append(np.nan)  # Append NaN if the row is problematic\n",
    "    averages = {key: np.nanmean(values) for key, values in scores.items()}  # Use np.nanmean to handle NaNs\n",
    "    return averages\n",
    "\n",
    "# Define the keys for Rouge, BERT, and Coherence scores\n",
    "rouge_keys = ['rouge1', 'rouge2', 'rougeL']\n",
    "bert_keys = ['bert_precision', 'bert_recall', 'bert_f1']\n",
    "coherence_keys = ['first_order_coherence', 'second_order_coherence']\n",
    "\n",
    "# Extract and calculate average scores for each approach using the corrected function\n",
    "markov_chain_avg_rouge = extract_average_scores_handling_nan(markov_chain_df, 'ROUGE Scores', rouge_keys)\n",
    "markov_chain_avg_bert = extract_average_scores_handling_nan(markov_chain_df, 'BERTScore', bert_keys)\n",
    "markov_chain_avg_coherence = extract_average_scores_handling_nan(markov_chain_df, 'Coherence Scores', coherence_keys)\n",
    "\n",
    "our_approach_avg_rouge = extract_average_scores_handling_nan(our_approach_df, 'ROUGE Scores', rouge_keys)\n",
    "our_approach_avg_bert = extract_average_scores_handling_nan(our_approach_df, 'BERTScore', bert_keys)\n",
    "our_approach_avg_coherence = extract_average_scores_handling_nan(our_approach_df, 'Coherence Scores', coherence_keys)\n",
    "\n",
    "llm_pure_avg_rouge = extract_average_scores_handling_nan(llm_pure_df, 'ROUGE Scores', rouge_keys)\n",
    "llm_pure_avg_bert = extract_average_scores_handling_nan(llm_pure_df, 'BERTScore', bert_keys)\n",
    "llm_pure_avg_coherence = extract_average_scores_handling_nan(llm_pure_df, 'Coherence Scores', coherence_keys)\n",
    "\n",
    "# Combine results into a summary dataframe for easier comparison\n",
    "summary_df = pd.DataFrame({\n",
    "    'Markov Chain': {**markov_chain_avg_rouge, **markov_chain_avg_bert, **markov_chain_avg_coherence},\n",
    "    'Our Approach': {**our_approach_avg_rouge, **our_approach_avg_bert, **our_approach_avg_coherence},\n",
    "    'LLM Pure': {**llm_pure_avg_rouge, **llm_pure_avg_bert, **llm_pure_avg_coherence}\n",
    "}).transpose()\n",
    "\n",
    "# Display the summary dataframe\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
